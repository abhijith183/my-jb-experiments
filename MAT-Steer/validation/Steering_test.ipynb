{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3915e04b-02f6-4167-b837-4f873927a24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb570c3b-b6de-44f1-bfc4-daf12cd0a182",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"truthfulqa\",\"toxigen\",\"bbq\"]\n",
    "\n",
    "num_attributes = len(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77117597-47e7-4297-b227-a9e3d55d5d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "756a67f0-a349-432b-bacb-ffe9ef8400f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"savemodelhere/model.pt\"\n",
    "\n",
    "os.makedirs(os.path.dirname(path), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f3a1aa7-cb97-4768-b62e-f34ffcc391a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"truthfulQA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "904f5346-83a5-4562-94c3-69299800b3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset truthfulQA: 108 positive, 100 negative examples\n"
     ]
    }
   ],
   "source": [
    "labels = np.load(f\"../features/mistral_tqa_labels.npy\")\n",
    "all_layer_wise_activations = np.load(f'../features/mistral_tqa_layer_wise.npy')\n",
    "pos_acts = torch.tensor(all_layer_wise_activations[labels == 1], dtype=torch.float32)\n",
    "neg_acts = torch.tensor(all_layer_wise_activations[labels == 0], dtype=torch.float32)\n",
    "tasks[dataset_name] = (pos_acts, neg_acts)\n",
    "\n",
    "print(f\"Dataset {dataset_name}: {pos_acts.shape[0]} positive, {neg_acts.shape[0]} negative examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adffbb78-9b9f-4124-bc41-0bae1c2cd7b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc788065-c659-4f15-800c-327e543fa72c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208, 4096)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_layer_wise_activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e14ab915-98e3-46e3-b412-e54a46dc80db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(x, y, sigma):\n",
    "    pairwise_dist = torch.cdist(x, y, p=2) ** 2\n",
    "    return torch.exp(-pairwise_dist / (2 * sigma ** 2))\n",
    "\n",
    "\n",
    "def compute_mmd(x, y, sigma):\n",
    "    K_xx = gaussian_kernel(x, x, sigma).mean()\n",
    "    K_yy = gaussian_kernel(y, y, sigma).mean()\n",
    "    K_xy = gaussian_kernel(x, y, sigma).mean()\n",
    "    return K_xx + K_yy - 2 * K_xy\n",
    "\n",
    "\n",
    "class SteeringModule(nn.Module):\n",
    "    def __init__(self, input_dim, num_attributes):\n",
    "        super(SteeringModule, self).__init__()\n",
    "        self.num_attributes = num_attributes\n",
    "        \n",
    "        # Steering vectors \n",
    "        self.steering_vectors = nn.ParameterList([\n",
    "            nn.Parameter(torch.randn(input_dim)) for _ in range(num_attributes)\n",
    "        ])\n",
    "        \n",
    "        # Gating function\n",
    "        self.gating_weights = nn.ModuleList([\n",
    "            nn.Linear(input_dim, 1) for _ in range(num_attributes)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, activations):\n",
    "        adjusted_activations = activations.clone()\n",
    "        gates = []\n",
    "        for t in range(self.num_attributes):\n",
    "            gate = torch.sigmoid(self.gating_weights[t](activations))\n",
    "            gates.append(gate)\n",
    "            adjusted_activations += gate * self.steering_vectors[t]\n",
    "        return adjusted_activations, torch.cat(gates, dim=1)\n",
    "\n",
    "\n",
    "def normalize_activations(original, adjusted):\n",
    "    \"\"\"Normalize the adjusted activations to preserve the original norm.\"\"\"\n",
    "    norm_original = torch.norm(original, p=2, dim=1, keepdim=True)\n",
    "    norm_adjusted = torch.norm(adjusted, p=2, dim=1, keepdim=True) + 1e-8  # Avoid division by zero\n",
    "    return adjusted * (norm_original / norm_adjusted)\n",
    "\n",
    "\n",
    "def sparsity_loss(gates):\n",
    "    \"\"\"Enforce sparsity in gating activations.\"\"\"\n",
    "    return torch.mean(torch.abs(gates))\n",
    "\n",
    "\n",
    "def orthogonality_loss(steering_vectors):\n",
    "    \"\"\"Steering vectors to be orthogonal to minimize interference.\"\"\"\n",
    "    loss = 0\n",
    "    num_vectors = len(steering_vectors)\n",
    "    for i in range(num_vectors):\n",
    "        for j in range(i + 1, num_vectors):\n",
    "            loss += (torch.dot(steering_vectors[i], steering_vectors[j]) / \n",
    "                     (torch.norm(steering_vectors[i]) * torch.norm(steering_vectors[j]))) ** 2\n",
    "    return loss\n",
    "\n",
    "\n",
    "def preservation_loss(gates):\n",
    "    \"\"\"Minimal intervention for positive activations.\"\"\"\n",
    "    return torch.mean((gates ** 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "557fd527-e882-45d7-8f2b-d7c43d8a32ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multi_task_steering(tasks, num_attributes, batch_size, epochs, lr, sigma, lambda_mmd, lambda_sparse, lambda_ortho, lambda_pos, save_path):\n",
    "    input_dim = list(tasks.values())[0][0].shape[1]  \n",
    "    model = SteeringModule(input_dim, num_attributes)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Prepare balanced sampling from each task\n",
    "    task_names = list(tasks.keys())\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        # Sample balanced mini-batches from each attribute\n",
    "        min_samples = min(min(tasks[task][0].shape[0], tasks[task][1].shape[0]) for task in tasks)\n",
    "        effective_batch_size = min(batch_size // (2 * num_attributes), min_samples)\n",
    "        \n",
    "        if effective_batch_size < 1:\n",
    "            effective_batch_size = 1\n",
    "        \n",
    "        # Create batches by sampling from each task\n",
    "        for batch_idx in range(0, min_samples, effective_batch_size):\n",
    "            batch_activations = []\n",
    "            batch_labels = []\n",
    "            batch_task_indices = []\n",
    "            \n",
    "            for t, task_name in enumerate(task_names):\n",
    "                pos_acts, neg_acts = tasks[task_name]\n",
    "                \n",
    "                # Sample positive and negative examples\n",
    "                pos_indices = torch.randperm(pos_acts.shape[0])[:effective_batch_size]\n",
    "                neg_indices = torch.randperm(neg_acts.shape[0])[:effective_batch_size]\n",
    "                \n",
    "                pos_batch = pos_acts[pos_indices]\n",
    "                neg_batch = neg_acts[neg_indices]\n",
    "                \n",
    "                batch_activations.append(pos_batch)\n",
    "                batch_activations.append(neg_batch)\n",
    "                \n",
    "                batch_labels.extend([1] * effective_batch_size)  # positive\n",
    "                batch_labels.extend([0] * effective_batch_size)  # negative\n",
    "                \n",
    "                batch_task_indices.extend([t] * effective_batch_size * 2)\n",
    "            \n",
    "            batch_activations = torch.cat(batch_activations, dim=0)\n",
    "            batch_labels = torch.tensor(batch_labels, dtype=torch.float32)\n",
    "            batch_task_indices = torch.tensor(batch_task_indices, dtype=torch.long)\n",
    "            \n",
    "            # Forward pass\n",
    "            adjusted_acts, gates = model(batch_activations)\n",
    "            adjusted_acts = normalize_activations(batch_activations, adjusted_acts)\n",
    "            \n",
    "            # Compute losses\n",
    "            total_loss = 0\n",
    "            \n",
    "            # MMD loss per attribute\n",
    "            loss_mmd = 0\n",
    "            for t, task_name in enumerate(task_names):\n",
    "                task_mask = batch_task_indices == t\n",
    "                if task_mask.sum() > 0:\n",
    "                    task_acts = adjusted_acts[task_mask]\n",
    "                    task_lbls = batch_labels[task_mask]\n",
    "                    \n",
    "                    pos_mask = task_lbls == 1\n",
    "                    neg_mask = task_lbls == 0\n",
    "                    \n",
    "                    if pos_mask.sum() > 0 and neg_mask.sum() > 0:\n",
    "                        pos_adjusted = task_acts[pos_mask]\n",
    "                        neg_adjusted = task_acts[neg_mask]\n",
    "                        \n",
    "                        # Compare adjusted negatives to original positives\n",
    "                        original_pos = tasks[task_name][0]\n",
    "                        sample_indices = torch.randperm(original_pos.shape[0])[:min(pos_adjusted.shape[0], original_pos.shape[0])]\n",
    "                        original_pos_sample = original_pos[sample_indices]\n",
    "                        \n",
    "                        loss_mmd += compute_mmd(neg_adjusted, original_pos_sample, sigma)\n",
    "            \n",
    "            loss_mmd = loss_mmd / num_attributes\n",
    "            \n",
    "            # Sparsity loss on negative examples\n",
    "            neg_mask = batch_labels == 0\n",
    "            if neg_mask.sum() > 0:\n",
    "                loss_sparse = sparsity_loss(gates[neg_mask])\n",
    "            else:\n",
    "                loss_sparse = torch.tensor(0.0)\n",
    "            \n",
    "            # Preservation loss on positive examples\n",
    "            pos_mask = batch_labels == 1\n",
    "            if pos_mask.sum() > 0:\n",
    "                loss_pos = preservation_loss(gates[pos_mask])\n",
    "            else:\n",
    "                loss_pos = torch.tensor(0.0)\n",
    "            \n",
    "            # Orthogonality loss\n",
    "            loss_ortho = orthogonality_loss([sv for sv in model.steering_vectors])\n",
    "            \n",
    "            # Combined loss\n",
    "            batch_loss = (lambda_mmd * loss_mmd + \n",
    "                         lambda_sparse * loss_sparse + \n",
    "                         lambda_ortho * loss_ortho + \n",
    "                         lambda_pos * loss_pos)\n",
    "            \n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            epoch_loss += batch_loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            avg_loss = epoch_loss / max(num_batches, 1)\n",
    "            print(f\"Epoch {epoch}, Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Save model with metadata\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'input_dim': input_dim,\n",
    "        'num_attributes': num_attributes,\n",
    "        'task_names': task_names\n",
    "    }\n",
    "    torch.save(checkpoint, save_path)\n",
    "    print(f\"Model saved to {save_path}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e738dd45-7320-4869-aa5d-740d27bec3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Average Loss: 1.3169\n",
      "Epoch 10, Average Loss: 0.1352\n",
      "Epoch 20, Average Loss: 0.0871\n",
      "Epoch 30, Average Loss: 0.0753\n",
      "Epoch 40, Average Loss: 0.0675\n",
      "Epoch 50, Average Loss: 0.0641\n",
      "Epoch 60, Average Loss: 0.0598\n",
      "Epoch 70, Average Loss: 0.0584\n",
      "Epoch 80, Average Loss: 0.0571\n",
      "Epoch 90, Average Loss: 0.0547\n",
      "Model saved to savemodelhere/model.pt\n"
     ]
    }
   ],
   "source": [
    "model = train_multi_task_steering(\n",
    "    tasks, 1, 96, 100, 0.001, 2.0, \n",
    "    1.0, 0.9, 0.1, 0.9, path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df7c262-e8dd-451a-be47-ac3952f5f940",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
