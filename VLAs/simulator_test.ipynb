{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f71cf46d-685f-48a1-a7ad-6f73785386a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29e8b4e1-ee97-4f58-9897-79b9d8f3a2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForVision2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ecc7a8f-c97e-4fce-b676-c4457a83622f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mNo private macro file found! (macros.py:53)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mIt is recommended to use a private macro file (macros.py:54)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mTo setup, run: python /anvil/projects/x-cis250308/miniconda/lib/python3.13/site-packages/robosuite/scripts/setup_macros.py (macros.py:55)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: make sure gym is installed if you want to use the GymWrapper.\n"
     ]
    }
   ],
   "source": [
    "import robosuite as suite\n",
    "from robosuite.controllers import load_controller_config\n",
    "from robosuite.wrappers import Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8602c216-0f93-4b4d-9597-ae771ad39f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 10:06:58.592411: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78002dce-e08d-4e1d-a059-ed79796c7457",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f887cf2e-e102-425e-9153-0af1b91a3b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"openvla/openvla-7b\",\n",
    "    trust_remote_code=True,\n",
    "    cache_dir = \"./vla_cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9bf989c-5679-4ced-9e06-01ea0c4ee712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vla = AutoModelForVision2Seq.from_pretrained(\n",
    "#     \"openvla/openvla-7b\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     trust_remote_code=True,\n",
    "#     low_cpu_mem_usage = True,\n",
    "#     cache_dir = \"./vla_cache\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e4b5986-525e-4c30-98e9-2d9f4e73a39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expected `transformers==4.40.1` and `tokenizers==0.19.1` but got `transformers==4.46.3` and `tokenizers==0.20.3`; there might be inference-time regressions due to dependency changes. If in doubt, pleaseuse the above versions.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39ace6fba3f74dcb8db53d9c2ef7594f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vla = AutoModelForVision2Seq.from_pretrained(\n",
    "    \"./vla_cache\",\n",
    "    local_files_only = True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage = True,\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6789873-0603-48d6-be33-ea929b5f2648",
   "metadata": {},
   "outputs": [],
   "source": [
    "vla = vla.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af79f668-c983-4d52-b13b-0006389ee971",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = suite.make(\n",
    "    \"Lift\",                     # simple manipulation task\n",
    "    robots=\"Panda\",             # Franka Panda 7-DoF arm\n",
    "    has_renderer=True,          # <-- GUI window ON\n",
    "    has_offscreen_renderer=True,\n",
    "    use_camera_obs=True,        # return camera obs\n",
    "    camera_names=[\"frontview\", \"birdview\"], # one camera is enough\n",
    "    camera_heights=224,\n",
    "    camera_widths=224,\n",
    "    render_camera=\"frontview\",  # show this in GUI\n",
    "    control_freq=20,            # 20 Hz control\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a057db7-f784-4e3d-887a-9289db3e7e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Wrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3de46723-7428-4e38-9f2a-1b194eae4a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Touch the cube\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "31f8cf59-f5d9-4dc9-a6b0-f710db2a825b",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a56d2a7-7c26-45f8-b827-612b83397e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import display\n",
    "# frame = obs[\"frontview_image\"]\n",
    "# prompt = \"take the cube\"\n",
    "# img = Image.fromarray(frame).transpose(method=Image.FLIP_TOP_BOTTOM)\n",
    "# display(img)\n",
    "# inps = processor(text = [prompt], images = [img]).to(device=device, dtype=torch.bfloat16)\n",
    "# with torch.no_grad():\n",
    "#     action = vla.predict_action(\n",
    "#         **inps,\n",
    "#         unnorm_key = \"bridge_orig\",\n",
    "#         do_sample = False\n",
    "#     )\n",
    "# action = action.tolist()\n",
    "# print(action)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0d59871d-5dbd-40d3-bbca-d4276fa3a345",
   "metadata": {},
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c26812c1-edfd-4e1c-b7b8-1ab0b2da9f1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'render'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m frame2 = obs[\u001b[33m\"\u001b[39m\u001b[33mbirdview_image\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      5\u001b[39m image2 = Image.fromarray(frame2)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m inputs = processor(text = [prompt], images = [image1]).to(device=device, dtype=torch.bfloat16)\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/anvil/projects/x-cis250308/miniconda/lib/python3.13/site-packages/robosuite/wrappers/wrapper.py:71\u001b[39m, in \u001b[36mWrapper.render\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs):\n\u001b[32m     65\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[33;03m    By default, run the normal environment render() function\u001b[39;00m\n\u001b[32m     67\u001b[39m \n\u001b[32m     68\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[33;03m        **kwargs (dict): Any args to pass to environment render function\u001b[39;00m\n\u001b[32m     70\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/anvil/projects/x-cis250308/miniconda/lib/python3.13/site-packages/robosuite/environments/base.py:450\u001b[39m, in \u001b[36mMujocoEnv.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    447\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    448\u001b[39m \u001b[33;03m    Renders to an on-screen window.\u001b[39;00m\n\u001b[32m    449\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m450\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mviewer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m()\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'render'"
     ]
    }
   ],
   "source": [
    "for step in range(200):\n",
    "    frame1 = obs[\"frontview_image\"]\n",
    "    image1 = Image.fromarray(frame1).transpose(method=Image.FLIP_TOP_BOTTOM)\n",
    "    # frame2 = obs[\"birdview_image\"]\n",
    "    # image2 = Image.fromarray(frame2)\n",
    "    env.render()\n",
    "    inputs = processor(text = [prompt], images = [image1]).to(device=device, dtype=torch.bfloat16)\n",
    "    with torch.no_grad():\n",
    "        action = vla.predict_action(\n",
    "            **inputs,\n",
    "            unnorm_key = 'bridge_orig',\n",
    "            do_sample=False\n",
    "        )\n",
    "    action = action.tolist()\n",
    "    action.append(action[-1])\n",
    "    obs, reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5f3a96b-a15d-4855-94a0-c3a8d3ab5ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888db1f0-a5d6-47e3-b0ff-a1af75670404",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
